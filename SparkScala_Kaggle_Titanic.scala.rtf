{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf600
{\fonttbl\f0\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\csgray\c0;}
\margl1440\margr1440\vieww24720\viewh9760\viewkind0
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0\fs26 \cf2 \CocoaLigature0 // Use the Titanic data set to get a Spark Scala Linear Regression going.\
// Use only the Age, Sex, Fare values for features.\
\
\
import org.apache.spark.sql.types.\{StructType, StructField, StringType, IntegerType, ArrayType, DoubleType\}\
\
// Get the training data.\
val traindata = spark.read.option("header","true").csv("/Users/fds/Downloads/titanic/train.csv")\
\
// Going to do multiple regression on Age, Sex, and Fare. \
// In the training data, only Age has null values. Find the average of Age so that null values can be replaced with it.\
// In the training set, this average ends up being 28.\
traindata.withColumn("AgeInt",col("Age").cast(IntegerType)).agg(avg("AgeInt")).show\
\
// Create the columns AgeInt, SexInt, FareDouble, which will be used to create the \'93features\'94 column for linear regression.\
// Also create the column \'93label\'94.\
val traindata10 = traindata.withColumn("AgeInt",when(col("Age").isNull, "28").otherwise(col("Age")).cast(IntegerType)).withColumn("SexInt",when(col("Sex").contains("female"),"1").otherwise("0").cast(IntegerType)).withColumn("FareDouble",col("Fare").cast(DoubleType)).withColumnRenamed("Survived","label") \
\
// Create the vector assembler and use it to create the \'93features\'94 column.\
val assembler10 = new VectorAssembler().setInputCols(Array("AgeInt","SexInt","FareDouble")).setOutputCol("features")\
val train10 = assembler10.transform(traindata10)\
\
// Create a Linear Regressor and train the model.\
val lr1000 = new LinearRegression().setMaxIter(1000)\
val tM = lr1000.fit(train10.select("label","features"))\
\
\
// Now that we have the trained model, we need to get the test data.\
val testdata = spark.read.option("header","true").csv("/Users/fds/Downloads/titanic/test.csv")\
\
// In the test data none of the Age or Sex values are null, but on Fare is null.\
// Find the average fare. I used the test data \'97 perhaps I should have used the training data?\
// The average fare is 35.6.\
testdata.agg(avg("Fare")).show\
\
// In two steps, transform the test data so that the VectorAssembler can be run on it.\
// Change the Age and Sex variables to integers, and handle the null Fare value.\
val test10 = testdata.withColumn("AgeInt",when(col("Age").isNull, "28").otherwise(col("Age")).cast(DoubleType)).withColumn("SexInt",when(col("Sex").contains("female"),"1").otherwise("0").cast(IntegerType)).withColumn("FareDouble",col("Fare").cast(DoubleType))\
val test10new = test10.withColumn("FareFinal",when(col("FareDouble").isNull,35.6).otherwise(col("FareDouble")))\
\
// For convenience, create a new assembler since we have a new column name, FareFinal.\
val assembler10new = new VectorAssembler().setInputCols(Array("AgeInt","SexInt","FareFinal")).setOutputCol("features")\
val testfinal = assembler10new.transform(test10new)\
\
// Now that we have testfinal, we can run the model tM on it and get the predictions. \
// Write out the predictions. If the probability is over 0.5, make it a survived. \
// Select just the columns PassengerId and Survived since that\'92s what the Kaggle submission wants.\
tM.transform(testfinal).withColumn("Survived",when(col("prediction")>0.5,1).otherwise(0)).select("PassengerId","Survived").write.csv("Survived")\
\
// This yielded a score of 0.76555, which is decent. \
\
\
\
\
}